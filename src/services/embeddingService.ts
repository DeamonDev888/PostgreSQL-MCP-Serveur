#!/usr/bin/env node

// import { OpenAI } from 'openai';
// import { pipeline } from '@xenova/transformers';
import Logger from '../utils/logger.js';

/**
 * Service de g√©n√©ration d'embeddings pour production
 * Supporte OpenAI API (1536 dims) et mod√®les locaux
 */
export class EmbeddingService {
  // private openai: OpenAI | null = null;
  // private localExtractor: any = null;
  private cache: Map<string, number[]> = new Map();
  private maxCacheSize = 1000;

  constructor() {
    // Initialiser OpenAI si cl√© disponible
    // if (process.env.OPENAI_API_KEY) {
    //   this.openai = new OpenAI({
    //     apiKey: process.env.OPENAI_API_KEY,
    //   });
    //   Logger.info('‚úÖ OpenAI embedding service initialis√©');
    // } else {
    //   Logger.warn('‚ö†Ô∏è OPENAI_API_KEY non trouv√© - utilisation du mode local uniquement');
    // }
    Logger.warn('‚ö†Ô∏è EmbeddingService - Mode mock (OpenAI et transformers non install√©s)');
  }

  /**
   * G√©n√®re un embedding √† partir d'un texte
   * @param text - Texte √† transformer en vecteur
   * @param options - Options de g√©n√©ration
   * @returns Promise<number[]> - Vecteur de 768 nombres
   */
  async generateEmbedding(
    text: string,
    options: {
      model?: 'openai' | 'local';
      useCache?: boolean;
      dimensions?: number;
    } = {}
  ): Promise<number[]> {
    const {
      model = 'local', // this.openai ? 'openai' : 'local',
      useCache = true,
      dimensions = 1536
    } = options;

    // Nettoyer et normaliser le texte
    const normalizedText = text.trim().toLowerCase();

    // V√©rifier le cache
    const cacheKey = `${model}:${normalizedText}`;
    if (useCache && this.cache.has(cacheKey)) {
      Logger.debug(`üì¶ Embedding r√©cup√©r√© du cache pour: "${text.substring(0, 50)}..."`);
      return this.cache.get(cacheKey)!;
    }

    try {
      let embedding: number[];

      // Mode mock - g√©n√©rer un vecteur al√©atoire pour les tests
      Logger.warn(`‚ö†Ô∏è Mode mock - g√©n√©ration vecteur al√©atoire pour: "${text.substring(0, 30)}..."`);
      embedding = this.generateMockEmbedding(dimensions);

      // V√©rifier les dimensions
      if (embedding.length !== dimensions) {
        throw new Error(`Dimension mismatch: expected ${dimensions}, got ${embedding.length}`);
      }

      // Ajouter au cache
      if (useCache) {
        this.addToCache(cacheKey, embedding);
      }

      Logger.debug(`‚úÖ Embedding g√©n√©r√© (${model}): ${embedding.length} dimensions`);
      return embedding;

    } catch (error: any) {
      Logger.error('‚ùå Erreur g√©n√©ration embedding:', error.message);
      throw new Error(`√âchec de g√©n√©ration d'embedding: ${error.message}`);
    }
  }

  /**
   * G√©n√®re un embedding mock (al√©atoire) pour les tests
   */
  private generateMockEmbedding(dimensions: number): number[] {
    const embedding: number[] = [];
    for (let i = 0; i < dimensions; i++) {
      embedding.push((Math.random() * 2) - 1);
    }
    // Normaliser
    const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    return embedding.map(val => val / magnitude);
  }

  // /**
  //  * G√©n√®re un embedding via OpenAI
  //  */
  // private async generateWithOpenAI(text: string): Promise<number[]> {
  //   if (!this.openai) {
  //     throw new Error('OpenAI non initialis√©');
  //   }

  //   const response = await this.openai.embeddings.create({
  //     model: 'text-embedding-3-small', // 768 dimensions
  //     input: text,
  //   });

  //   return response.data[0].embedding;
  // }

  // /**
  //  * G√©n√®re un embedding via mod√®le local
  //  */
  // private async generateWithLocal(text: string): Promise<number[]> {
  //   if (!this.localExtractor) {
  //     Logger.info('üîÑ Initialisation du mod√®le local (peut prendre du temps)...');
  //     this.localExtractor = await pipeline(
  //       'feature-extraction',
  //       'Xenova/all-mpnet-base-v2'
  //     );
  //     Logger.info('‚úÖ Mod√®le local charg√©');
  //   }

  //   const output = await this.localExtractor(text);
  //   return Array.from(output.data);
  // }

  /**
   * Ajoute un embedding au cache avec gestion de la taille
   */
  private addToCache(key: string, embedding: number[]): void {
    if (this.cache.size >= this.maxCacheSize) {
      // Supprimer l'entr√©e la plus ancienne
      const firstKey = this.cache.keys().next().value;
      if (firstKey !== undefined) {
        this.cache.delete(firstKey);
        Logger.debug('üóëÔ∏è Cache plein - entr√©e la plus ancienne supprim√©e');
      }
    }

    this.cache.set(key, embedding);
  }

  /**
   * Vide le cache
   */
  clearCache(): void {
    this.cache.clear();
    Logger.info('üßπ Cache d\'embeddings vid√©');
  }

  /**
   * Statistiques du cache
   */
  getCacheStats(): { size: number; maxSize: number; hitRate?: number } {
    return {
      size: this.cache.size,
      maxSize: this.maxCacheSize,
    };
  }

  /**
   * Test de performance du service
   */
  async benchmark(texts: string[]): Promise<{
    averageTime: number;
    totalTime: number;
    successRate: number;
  }> {
    Logger.info(`üß™ Benchmark sur ${texts.length} textes...`);

    const startTime = Date.now();
    let successCount = 0;

    for (const text of texts) {
      try {
        await this.generateEmbedding(text);
        successCount++;
      } catch (error) {
        Logger.error(`‚ùå √âchec pour: "${text.substring(0, 30)}..."`);
      }
    }

    const totalTime = Date.now() - startTime;
    const averageTime = totalTime / texts.length;
    const successRate = (successCount / texts.length) * 100;

    Logger.info(`‚úÖ Benchmark termin√©:`);
    Logger.info(`   ‚Ä¢ Temps moyen: ${averageTime.toFixed(2)}ms`);
    Logger.info(`   ‚Ä¢ Taux de succ√®s: ${successRate.toFixed(1)}%`);

    return {
      averageTime,
      totalTime,
      successRate,
    };
  }
}

// Export singleton
export const embeddingService = new EmbeddingService();
